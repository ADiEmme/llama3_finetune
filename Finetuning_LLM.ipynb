{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ADiEmme/llama3_finetune/blob/main/Finetuning_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK6O5HKRZRek"
   },
   "source": [
    "# Addestra un LLM sui tuoi dati - Tutorial Simone Rizzo\n",
    "Questo notebook Ã¨ una demo per dimostrare come si puÃ² addestrare un LLM open-source privato sui propri dati per poi esportarlo in GGFU per Ollama.\n",
    "\n",
    "**Nota bene**: se vuoi usare la GPU gratuita vai da Modifica -> Impostazioni blocco note -> T4 GPU\n",
    "\n",
    "La demo Ã¨ stata realizzata da **Simone Rizzo**:\n",
    "- [Youtube](https://www.youtube.com/channel/UCbMlkb79E12CwveGAtdFj-A)\n",
    "- [Linkedin](https://www.linkedin.com/in/simone-rizzo-9851b7147/)\n",
    "- [TikTok](https://www.tiktok.com/@simonerizzo98)\n",
    "- [Instagram](https://www.instagram.com/simorizzo_ai/)\n",
    "\n",
    "Seguimi e lascia un like sui miei social ðŸ˜œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwppO_fDYXaM"
   },
   "source": [
    "# Installazione delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRstH74nKOoF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWVFhrAPYiwT"
   },
   "source": [
    "# Inizializzazione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "37e0513e5fff472599e737f32e9e0db0",
      "659f749709e34336a56ca45e1b36f3fa",
      "b595a01fb4ca40a8bbf35468b9fe0f1d",
      "21a7666d8eee4fb6ab52c7e47109d64b",
      "74933b84532e4c299d63e1de83dddc44",
      "77871b586914442aa12dfe5c2de85ffb",
      "dfd1717625a249df8a6f4f254f747359",
      "c7004356292f44f4b5997a632e96dde0",
      "60f7296a54134800bd09b5ef2d48c432",
      "402348b06aba4efdace6bbfdf79c190d",
      "86644c70735845b09437ba9984c06bee",
      "8529674407c04e8b86e0c3416a1515a3",
      "04bb375634d1402ea7d26e53b1d24fc3",
      "cfe82295414a47daa805d814a5b97c65",
      "d5b21c4b573345419950230fd588640c",
      "c6a2e12f87a6417c85fbd7c3e99c7bd3",
      "f7a882afe79041a3a6c69affe9243acd",
      "3633d6376bf2496e93f28166d9b3fac8",
      "3fe9e1dfede643f3bc26e87e327b5007",
      "b06c786bcd1a43e48513b7586c54dec1",
      "64d2304c056c4939ab8e27c05448fe7a",
      "0719ae3b16cf414582ae4c383e06d5bc",
      "fe417336598a48a7b62bb2ca60b5273b",
      "9ff602f23fc3402a8ee31ce8754d7ba3",
      "e7180a691cdc413c9a8f3f17fd43e749",
      "43580b588a2347579ff0c4e2c085cff5",
      "b568538f20c540b3bea38009c7a4b571",
      "c2d3600aad3d44cea46e597d5f0d308e",
      "922c673045bc48aa8ab46f96ad704e82",
      "b965c71b46a549989ba215f9902d76d4",
      "beafe20a93ac46fcbbd8b0dbbccf18c2",
      "3c09c33668f645dd9cf3e814c942aca1",
      "c555a9f5eab64298914c91e9d2ca05eb",
      "4160533448c64083a7669e36a9593c28",
      "b4af2e636a254b0ab60510ae6abe7e0d",
      "56c73427e1f1491eba709a7da33074dd",
      "28ba96314622458ab907ae90b8eb2969",
      "95150ab2eeca4f0d85748ed0f5f76986",
      "9664324d98ad4b55b58d7d3fe652ebab",
      "de94a9c31a634692943ef311567f1da4",
      "1d1a4c776e3944a1a7320edb3aa4e7e2",
      "7cd686bb24024ce69d380a622220ef72",
      "542e13fad4b744219c424e1993888944",
      "ce4f2ca1794649fe9dfc043ebe6fb8d6",
      "bda89e0d099e41f19110db88d25db97e",
      "8e91c3d5d80142ce8aee3bdd0ba8dcbb",
      "e538504e96de4e8a935ef855d5fce393",
      "c6cd8f8300454944b1f25b9b96d25393",
      "4f9db5e3eed94f5fa34d56b18e466799",
      "93f8b64b79e242e59d74cdde4b542e42",
      "892625fe5bfd4c7c834b0aa737e320ae",
      "61d4b8791ca842e3b51c458141ea6f08",
      "9f7d8f2639604547897ea87e18e6263b",
      "f72472a761014b7d94225c3d10c048e3",
      "666b4d2af94a4a9da9e42718982752ce"
     ]
    },
    "id": "gEZQj3dRKhBU",
    "outputId": "af22d77e-ddd8-4b13-cb82-15600f56e2ac"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGsV4_uxLG5S",
    "outputId": "22f078cd-1945-4d21-f5f9-33a26e43db3c"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z28N8-D-Y_Av",
    "outputId": "7d1b49b4-7246-4b84-ca2b-4c0de1bea254"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"flux prompt for a Viking warrior\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wajMndGtYfBE"
   },
   "source": [
    "# Formattazione del testo per il formato di LLama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZjS3uEALn8Q"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqDs4RHMjalj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/content/flux-training-dataset (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "Nt34mazGDNKF",
    "outputId": "025fb6d1-81cf-4b1a-a163-5a54e93926f4"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNNZTgWekQoU"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPNzoSnvlkws"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "df[\"conversations\"] = df.apply(\n",
    "    lambda x: [\n",
    "        {\"content\": x[\"User\"], \"role\": \"user\"},\n",
    "        {\"content\": x[\"Prompt\"], \"role\": \"assistant\"}\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "# Ora convertiamo il DataFrame in un Dataset di HuggingFace, rimuovendo le vecchie colonne\n",
    "dataset = Dataset.from_pandas(df.drop(columns=[\"User\", \"Prompt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lw5I1qju3qbk",
    "outputId": "88579b72-aad2-4ef5-9613-70d9c3c2e228"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NK2pLXh6D_ZV",
    "outputId": "8ca8af68-25e6-487e-b5e1-8224250420b9"
   },
   "outputs": [],
   "source": [
    "dataset['conversations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "d59dd686908b4abf9fabb7594c8a3ee4",
      "ebf1024f690346e082aa2d6041806e54",
      "4e12a841ac364dc898f417706f6099b4",
      "fbff4dd167404464989846e4c4706df4",
      "3ae013a36fb44a2ea28c7c809dbb3ee4",
      "7b986230fe464140ada3d7201426e96b",
      "f84d9e1c66fa47bda7568553d3cbd607",
      "176552c30b1c4aaabd3230bb77e66a91",
      "58b50a4ff5be4e16b0e43bd371cb3f84",
      "0a2d8ca507454f508452b43817da3a50",
      "d8f4580cdb4e4db5b4843d57dbea10e6",
      "0342782f24af49b3ba3953be263dab6d",
      "73d074bd34654675a70ea4cdf01d8a69",
      "b914895b39844be59ec002fdc7daafbd",
      "3a498f9e0e3049d4a16f8e57f672fc24",
      "eefd911319234790abc5301709a1e003",
      "4b10f169804d42bcb8a5b313ac4466dc",
      "9de048ab90cf42f8904bf9d2a4b5a478",
      "d0452bba4bf74ec8928940a108f6a923",
      "31afdc7f710f416c8c84fd04408a4f8b",
      "74e3b5bfec68452b883c79f97abb7f53",
      "1dde2ef6174243aaa1d3d60ba827a7f3"
     ]
    },
    "id": "0jTy93cq3TiX",
    "outputId": "a4d5c3c0-0fc3-4601-f870-f7593fe1938b"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzlqxG_h3Zws",
    "outputId": "04a5fa02-6a55-4c8e-cbad-42f5c4089419"
   },
   "outputs": [],
   "source": [
    "dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "DQRNht_t3dX7",
    "outputId": "93529094-4e18-4356-a247-5a1dea307fe4"
   },
   "outputs": [],
   "source": [
    "dataset[5][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt6N-6NUPo6j"
   },
   "source": [
    "# Addestramento del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "a78e693f77424f428bd617d4984535ff",
      "9f434d06fb484feebcc429bb19a9e942",
      "37a86bd6347340f892cdbc358868309e",
      "ba64d5b2df534cd1b792ecd9dee6270a",
      "f0d57b05c1934f7abb951f6f9d3332c1",
      "3a2bbdc1cf294cada63a9c35203791de",
      "99be6b424ee24a67832638fb6b658e41",
      "9b30a7eeb4244c66b5b103f9fac5d2a6",
      "543d14620b634af08c0466ffd1e29b83",
      "3a887e269e5e426497ed7e63c7070454",
      "2669f49876b74f2ab93aa72d94150f68"
     ]
    },
    "id": "_M82MLCtPoWZ",
    "outputId": "e696415b-e334-4fdc-a764-16cc2dbd33c1"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 50,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c13943643dfc4a089651734901d2af80",
      "c6b6537764c24c9c97607255459a2f52",
      "7cd3e77f1c5444d7941f83fb9e714b35",
      "67a8eb84d0264dcaab4fc91fbaa07725",
      "e9857bf2bba1431cabc632d309c79626",
      "088e500982ec44f6a49e903dcae2c4f8",
      "ffafc5453b03475b9be137833d4bdbb1",
      "1538d2628d184fb7b093dc4c06e8477d",
      "ac119de1e7bc472396b004b62ccb9ce8",
      "20d23bf7748e49c5b5dd7c1b15eb41e0",
      "323fe6a74aea4686a834dac5d584aa72"
     ]
    },
    "id": "AymD3xqeP6T4",
    "outputId": "651fa1ba-8730-4885-91f9-bb3d5a72e839"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgIrL2V6QWLq",
    "outputId": "eaf66fd0-bd57-4d3e-f2c7-ae8183d79ef8"
   },
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hfaZRnqfQY_n",
    "outputId": "978050ce-34b1-4081-8be2-de01d4b922b4"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skufO15UQkVL"
   },
   "source": [
    "# Inferenza con Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJVtRU40Ql7Y",
    "outputId": "6a248c24-ba8d-4a61-c601-0351a8c1f556"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"flux prompt for a Viking warrior with a black iron amulet\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSW_4JFqUL4p"
   },
   "source": [
    "# Esporto in GGUF per Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GU9qlQ6qUfiY"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"/content/drive/My Drive/model\", tokenizer, quantization_method = \"f16\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "\n",
    "# Save to 8bit Q8_0\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4y3pp4fIXF-i",
    "outputId": "25c9b6d0-73a9-43e7-e4dd-b6fd5c6ea939"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('/content/model/unsloth.F16.gguf')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
